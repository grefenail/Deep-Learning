{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a7b949d175fd35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T18:30:56.485707Z",
     "start_time": "2025-02-15T18:30:56.483549Z"
    }
   },
   "source": [
    "# increase gpu memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac14489",
   "metadata": {},
   "source": [
    "If you face disk quota exceeded, try running this cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77dedef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 10:12:02.326321: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740301922.361068  714330 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740301922.370638  714330 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers.utils import logging\n",
    "\n",
    "# Limit GPU memory usage\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\"\n",
    "\n",
    "# Enable memory growth for TensorFlow\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f7a86",
   "metadata": {},
   "source": [
    "# Finetune on FashionPedia  (frozone resnet backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f130b7c",
   "metadata": {},
   "source": [
    "For this experiment we refered : https://huggingface.co/learn/cookbook/fine_tuning_detr_custom_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703820e",
   "metadata": {},
   "source": [
    "Download dataset: https://huggingface.co/datasets/detection-datasets/fashionpedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d32be",
   "metadata": {},
   "source": [
    "NOTE: GenAI tools were used for debugging and codinf assistance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ce071",
   "metadata": {},
   "source": [
    "1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7782a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import io\n",
    "import torch\n",
    "import albumentations as A\n",
    "from transformers import AutoImageProcessor\n",
    "from transformers import AutoModelForObjectDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from prettytable import PrettyTable\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83caae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"parquet\", data_dir=\"/home/utn/firi22ka/Desktop/jenga/mlp/g10/fashionpedia/data\", cache_dir=\"/home/utn/firi22ka/Desktop/jenga/hf_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66aeb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56a2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e824af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below code  to select  10,000 imagee for training and 1000 images for testing\n",
    "# train_subset = dataset[\"train\"].select(range(10000))\n",
    "test_subset = dataset[\"validation\"].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92acc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# converting id to lables\n",
    "id2label = {\n",
    "    0: \"shirt, blouse\",\n",
    "    1: \"top, t-shirt, sweatshirt\",\n",
    "    2: \"sweater\",\n",
    "    3: \"cardigan\",\n",
    "    4: \"jacket\",\n",
    "    5: \"vest\",\n",
    "    6: \"pants\",\n",
    "    7: \"shorts\",\n",
    "    8: \"skirt\",\n",
    "    9: \"coat\",\n",
    "    10: \"dress\",\n",
    "    11: \"jumpsuit\",\n",
    "    12: \"cape\",\n",
    "    13: \"glasses\",\n",
    "    14: \"hat\",\n",
    "    15: \"headband, head covering, hair accessory\",\n",
    "    16: \"tie\",\n",
    "    17: \"glove\",\n",
    "    18: \"watch\",\n",
    "    19: \"belt\",\n",
    "    20: \"leg warmer\",\n",
    "    21: \"tights, stockings\",\n",
    "    22: \"sock\",\n",
    "    23: \"shoe\",\n",
    "    24: \"bag, wallet\",\n",
    "    25: \"scarf\",\n",
    "    26: \"umbrella\",\n",
    "    27: \"hood\",\n",
    "    28: \"collar\",\n",
    "    29: \"lapel\",\n",
    "    30: \"epaulette\",\n",
    "    31: \"sleeve\",\n",
    "    32: \"pocket\",\n",
    "    33: \"neckline\",\n",
    "    34: \"buckle\",\n",
    "    35: \"zipper\",\n",
    "    36: \"applique\",\n",
    "    37: \"bead\",\n",
    "    38: \"bow\",\n",
    "    39: \"flower\",\n",
    "    40: \"fringe\",\n",
    "    41: \"ribbon\",\n",
    "    42: \"rivet\",\n",
    "    43: \"ruffle\",\n",
    "    44: \"sequin\",\n",
    "    45: \"tassel\",\n",
    "}\n",
    "\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e959c7",
   "metadata": {},
   "source": [
    "Visualizing a sample image from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21110e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_image_from_idx(dataset, idx):\n",
    "    sample = dataset[idx]\n",
    "    image = sample[\"image\"]\n",
    "    image_bytes = image[\"bytes\"]  # Extract bytes from dictionary\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    # print(image)  \n",
    "    annotations = sample[\"objects\"]\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    width, height = sample[\"width\"], sample[\"height\"]\n",
    "\n",
    "    print(annotations)\n",
    "\n",
    "    for i in range(len(annotations[\"bbox_id\"])):\n",
    "        box = annotations[\"bbox\"][i]\n",
    "        x1, y1, x2, y2 = tuple(box)\n",
    "        draw.rectangle((x1, y1, x2, y2), outline=\"red\", width=3)\n",
    "        draw.text((x1, y1), id2label[annotations[\"category\"][i]], fill=\"green\")\n",
    "\n",
    "    return image\n",
    "\n",
    " \n",
    "# draw_image_from_idx(dataset=test_dataset, idx=200)  # You can test changing this id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41131061",
   "metadata": {},
   "source": [
    "Data Filtering: removing invalid bounding boxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4f9edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_invalid_bboxes(example):\n",
    "    valid_bboxes = []\n",
    "    valid_bbox_ids = []\n",
    "    valid_categories = []\n",
    "    valid_areas = []\n",
    "\n",
    "    for i, bbox in enumerate(example[\"objects\"][\"bbox\"]):\n",
    "        x_min, y_min, x_max, y_max = bbox[:4]\n",
    "        if x_min < x_max and y_min < y_max:\n",
    "            valid_bboxes.append(bbox)\n",
    "            valid_bbox_ids.append(example[\"objects\"][\"bbox_id\"][i])\n",
    "            valid_categories.append(example[\"objects\"][\"category\"][i])\n",
    "            valid_areas.append(example[\"objects\"][\"area\"][i])\n",
    "        else:\n",
    "            print(\n",
    "                f\"Image with invalid bbox: {example['image_id']} Invalid bbox detected and discarded: {bbox} - bbox_id: {example['objects']['bbox_id'][i]} - category: {example['objects']['category'][i]}\"\n",
    "            )\n",
    "\n",
    "    example[\"objects\"][\"bbox\"] = valid_bboxes\n",
    "    example[\"objects\"][\"bbox_id\"] = valid_bbox_ids\n",
    "    example[\"objects\"][\"category\"] = valid_categories\n",
    "    example[\"objects\"][\"area\"] = valid_areas\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(filter_invalid_bboxes)\n",
    "# test_dataset = test_dataset.map(filter_invalid_bboxes)\n",
    "test_dataset = test_subset.map(filter_invalid_bboxes) #test on 1000 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "019e12ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image_id', 'image', 'width', 'height', 'objects'],\n",
      "    num_rows: 45623\n",
      "})\n",
      "Dataset({\n",
      "    features: ['image_id', 'image', 'width', 'height', 'objects'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb500d0",
   "metadata": {},
   "source": [
    "Data Augmentation: Applying data augmentation techniques for better data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a914f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_714330/2247774584.py:6: UserWarning: Argument(s) 'value' are not valid for transform PadIfNeeded\n",
      "  A.PadIfNeeded(500, 500, border_mode=0, value=(0, 0, 0)),\n",
      "/tmp/ipykernel_714330/2247774584.py:21: UserWarning: Argument(s) 'value' are not valid for transform PadIfNeeded\n",
      "  A.PadIfNeeded(500, 500, border_mode=0, value=(0, 0, 0)),\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(500),\n",
    "        A.PadIfNeeded(500, 500, border_mode=0, value=(0, 0, 0)),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.HueSaturationValue(p=0.5),\n",
    "        A.Rotate(limit=10, p=0.5),\n",
    "        A.RandomScale(scale_limit=0.2, p=0.5),\n",
    "        A.GaussianBlur(p=0.5),\n",
    "        A.GaussNoise(p=0.5),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"category\"]),\n",
    ")\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(500),\n",
    "        A.PadIfNeeded(500, 500, border_mode=0, value=(0, 0, 0)),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"category\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7cd5c",
   "metadata": {},
   "source": [
    "Loading pretrained weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efcd24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the models you want to train on or test on , and comment out other current chekpoint\n",
    "# checkpoint = \"/home/utn/firi22ka/Desktop/jenga/mlp/g10/test/detr-resnet-50-fashionpedia-full-finetuned/checkpoint-5000\" \n",
    "# checkpoint = '/home/utn/firi22ka/Desktop/jenga/mlp/g10/test/detr-resnet-50-fashionpedia-full-finetuned_10000/checkpoint-12500'\n",
    "checkpoint = '/home/utn/firi22ka/Desktop/jenga/mlp/g10/test/detr-resnet-50-fashionpedia-full-finetuned_5epochs/checkpoint-25000'\n",
    "# checkpoint = \"facebook/detr-resnet-50\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3fc589",
   "metadata": {},
   "source": [
    "Converting data into DeRT compatible fromat for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b31520c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_anns(image_id, category, area, bbox):\n",
    "    annotations = []\n",
    "    for i in range(0, len(category)):\n",
    "        new_ann = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category[i],\n",
    "            \"isCrowd\": 0,\n",
    "            \"area\": area[i],\n",
    "            \"bbox\": list(bbox[i]),\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def convert_voc_to_coco(bbox):\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    width = xmax - xmin\n",
    "    height = ymax - ymin\n",
    "    return [xmin, ymin, width, height]\n",
    "\n",
    "\n",
    "def transform_aug_ann(examples, transform):\n",
    "    image_ids = examples[\"image_id\"]\n",
    "    images, bboxes, area, categories = [], [], [], []\n",
    "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
    "        if isinstance(image, dict) and \"bytes\" in image:\n",
    "            image = Image.open(io.BytesIO(image[\"bytes\"]))\n",
    "        \n",
    "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
    "        out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n",
    "\n",
    "        area.append(objects[\"area\"])\n",
    "        images.append(out[\"image\"])\n",
    "\n",
    "        # Convert to COCO format\n",
    "        converted_bboxes = [convert_voc_to_coco(bbox) for bbox in out[\"bboxes\"]]\n",
    "        bboxes.append(converted_bboxes)\n",
    "\n",
    "        categories.append(out[\"category\"])\n",
    "\n",
    "    targets = [\n",
    "        {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n",
    "        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n",
    "    ]\n",
    "\n",
    "    return image_processor(images=images, annotations=targets, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "def transform_train(examples):\n",
    "    return transform_aug_ann(examples, transform=train_transform)\n",
    "\n",
    "\n",
    "def transform_val(examples):\n",
    "    return transform_aug_ann(examples, transform=val_transform)\n",
    "\n",
    "\n",
    "train_dataset_transformed = train_dataset.with_transform(transform_train)\n",
    "test_dataset_transformed = test_dataset.with_transform(transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27e5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Updated draw function to accept an optional transform\n",
    "def draw_augmented_image_from_idx(dataset, idx, transform=None):\n",
    "    sample = dataset[idx]\n",
    "    image = sample[\"image\"]\n",
    "    image_bytes = image[\"bytes\"]  # Extract bytes from dictionary\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    annotations = sample[\"objects\"]\n",
    "\n",
    "    # Convert image to RGB and NumPy array\n",
    "    image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
    "\n",
    "    if transform:\n",
    "        augmented = transform(image=image, bboxes=annotations[\"bbox\"], category=annotations[\"category\"])\n",
    "        image = augmented[\"image\"]\n",
    "        annotations[\"bbox\"] = augmented[\"bboxes\"]\n",
    "        annotations[\"category\"] = augmented[\"category\"]\n",
    "\n",
    "    image = Image.fromarray(image[:, :, ::-1])  # Convert back to PIL Image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    width, height = sample[\"width\"], sample[\"height\"]\n",
    "\n",
    "    for i in range(len(annotations[\"bbox_id\"])):\n",
    "        box = annotations[\"bbox\"][i]\n",
    "        x1, y1, x2, y2 = tuple(box)\n",
    "\n",
    "        # Normalizing  coordinates\n",
    "        if max(box) <= 1.0:\n",
    "            x1, y1 = int(x1 * width), int(y1 * height)\n",
    "            x2, y2 = int(x2 * width), int(y2 * height)\n",
    "        else:\n",
    "            x1, y1 = int(x1), int(y1)\n",
    "            x2, y2 = int(x2), int(y2)\n",
    "\n",
    "        draw.rectangle((x1, y1, x2, y2), outline=\"red\", width=3)\n",
    "        draw.text((x1, y1), id2label[annotations[\"category\"][i]], fill=\"green\")\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def plot_augmented_images(dataset, indices, transform=None):\n",
    "    \"\"\"\n",
    "    Plot images and their annotations with optional augmentation.\n",
    "    \"\"\"\n",
    "    num_rows = len(indices) // 3\n",
    "    num_cols = 3\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "\n",
    "        # Draw augmented image\n",
    "        image = draw_augmented_image_from_idx(dataset, idx, transform=transform)\n",
    "\n",
    "        # Display image on the corresponding subplot\n",
    "        axes[row, col].imshow(image)\n",
    "        axes[row, col].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plotting augmented images\n",
    "# plot_augmented_images(train_dataset, range(9), transform=train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e349c3",
   "metadata": {},
   "source": [
    "# Fine-tuning DeRT on complete dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea4364",
   "metadata": {},
   "source": [
    "Initializing model and save directory for fintuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d20af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "output_dir = \"detr-resnet-50-fashionpedia-full-finetuned_5epochs\"  # edit this to save fine-tuned model in your choice of directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a414f2",
   "metadata": {},
   "source": [
    "Uncomment below cell and tun if you want to finetune dert by freezing backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581663dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = AutoModelForObjectDetection.from_pretrained(\n",
    "#     checkpoint,\n",
    "#     id2label=id2label,\n",
    "#     label2id=label2id,\n",
    "#     ignore_mismatched_sizes=True,\n",
    "# )\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"backbone\" in name:\n",
    "#         param.requires_grad = False\n",
    "#         print(f\"Frozen parameter: {name}\")\n",
    "\n",
    "# # Print trainable parameters\n",
    "# print(\"\\nTrainable parameters:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(f\"{name}: {param.shape}\")\n",
    "# output_dir = \"detr_finetuned_coco_backobone_freeze_100\"  # edit this to save fine-tuned model in your choice of directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40056b4a",
   "metadata": {},
   "source": [
    "Defining  training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e11f8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utn/firi22ka/Desktop/jenga/mlp/g10/vit_env/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    logging_steps=1,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=1e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    batch_eval_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec97a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def denormalize_boxes(boxes, width, height):\n",
    "    boxes = boxes.clone()\n",
    "    boxes[:, 0] *= width  # xmin\n",
    "    boxes[:, 1] *= height  # ymin\n",
    "    boxes[:, 2] *= width  # xmax\n",
    "    boxes[:, 3] *= height  # ymax\n",
    "    return boxes\n",
    "\n",
    "\n",
    "batch_metrics = []\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred, compute_result):\n",
    "    global batch_metrics\n",
    "\n",
    "    (loss_dict, scores, pred_boxes, last_hidden_state, encoder_last_hidden_state), labels = eval_pred\n",
    "\n",
    "    image_sizes = []\n",
    "    target = []\n",
    "    for label in labels:\n",
    "\n",
    "        image_sizes.append(label[\"orig_size\"])\n",
    "        width, height = label[\"orig_size\"]\n",
    "        denormalized_boxes = denormalize_boxes(label[\"boxes\"], width, height)\n",
    "        target.append(\n",
    "            {\n",
    "                \"boxes\": denormalized_boxes,\n",
    "                \"labels\": label[\"class_labels\"],\n",
    "            }\n",
    "        )\n",
    "    predictions = []\n",
    "    for score, box, target_sizes in zip(scores, pred_boxes, image_sizes):\n",
    "        # Extract the bounding boxes, labels, and scores from the model's output\n",
    "        pred_scores = score[:, :-1]  # Exclude the no-object class\n",
    "        pred_scores = softmax(pred_scores, dim=-1)\n",
    "        width, height = target_sizes\n",
    "        pred_boxes = denormalize_boxes(box, width, height)\n",
    "        pred_labels = torch.argmax(pred_scores, dim=-1)\n",
    "\n",
    "        # Get the scores corresponding to the predicted labels\n",
    "        pred_scores_for_labels = torch.gather(pred_scores, 1, pred_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        predictions.append(\n",
    "            {\n",
    "                \"boxes\": pred_boxes,\n",
    "                \"scores\": pred_scores_for_labels,\n",
    "                \"labels\": pred_labels,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    metric = MeanAveragePrecision(box_format=\"xywh\", class_metrics=True)\n",
    "\n",
    "    if not compute_result:\n",
    "        # Accumulate batch-level metrics\n",
    "        batch_metrics.append({\"preds\": predictions, \"target\": target})\n",
    "        return {}\n",
    "    else:\n",
    "        # Compute final aggregated metrics\n",
    "        # Aggregate batch-level metrics (this should be done based on your metric library's needs)\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        for batch in batch_metrics:\n",
    "            all_preds.extend(batch[\"preds\"])\n",
    "            all_targets.extend(batch[\"target\"])\n",
    "\n",
    "        # Update metric with all accumulated predictions and targets\n",
    "        metric.update(preds=all_preds, target=all_targets)\n",
    "        metrics = metric.compute()\n",
    "\n",
    "        # Convert and format metrics as needed\n",
    "        classes = metrics.pop(\"classes\")\n",
    "        map_per_class = metrics.pop(\"map_per_class\")\n",
    "        mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n",
    "\n",
    "        for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n",
    "            class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n",
    "            metrics[f\"map_{class_name}\"] = class_map\n",
    "            metrics[f\"mar_100_{class_name}\"] = class_mar\n",
    "\n",
    "        # Round metrics for cleaner output\n",
    "        metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
    "\n",
    "        # Clear batch metrics for next evaluation\n",
    "        batch_metrics = []\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a96a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4256996f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_714330/3435597507.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_dataset_transformed,\n",
    "    eval_dataset=test_dataset_transformed,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# uncomment below line to start training using pretrained weights\n",
    "# trainer.train()\n",
    "\n",
    "#uncomment below line to start training from a fine-tuned checkpoint\n",
    "# trainer.train(resume_from_checkpoint= \"/home/utn/firi22ka/Desktop/jenga/mlp/g10/test/detr-resnet-50-fashionpedia-full-finetuned_5epochs/checkpoint-16400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this if you fet disk quota exceeded error. \n",
    "# !export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3652b",
   "metadata": {},
   "source": [
    "Evaluate your fine-tuned model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbf727de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9507602453231812, 'eval_model_preparation_time': 0.0025, 'eval_map': 0.0436, 'eval_map_50': 0.0812, 'eval_map_75': 0.0419, 'eval_map_small': 0.0167, 'eval_map_medium': 0.0464, 'eval_map_large': 0.0732, 'eval_mar_1': 0.0894, 'eval_mar_10': 0.1299, 'eval_mar_100': 0.1315, 'eval_mar_small': 0.0458, 'eval_mar_medium': 0.1432, 'eval_mar_large': 0.166, 'eval_map_shirt, blouse': 0.0069, 'eval_mar_100_shirt, blouse': 0.0368, 'eval_map_top, t-shirt, sweatshirt': 0.0876, 'eval_mar_100_top, t-shirt, sweatshirt': 0.4731, 'eval_map_sweater': 0.0, 'eval_mar_100_sweater': 0.0, 'eval_map_cardigan': 0.0, 'eval_mar_100_cardigan': 0.0, 'eval_map_jacket': 0.1701, 'eval_mar_100_jacket': 0.6348, 'eval_map_vest': 0.0, 'eval_mar_100_vest': 0.0, 'eval_map_pants': 0.3809, 'eval_mar_100_pants': 0.677, 'eval_map_shorts': 0.0353, 'eval_mar_100_shorts': 0.0904, 'eval_map_skirt': 0.0765, 'eval_mar_100_skirt': 0.4514, 'eval_map_coat': 0.0, 'eval_mar_100_coat': 0.0, 'eval_map_dress': 0.3374, 'eval_mar_100_dress': 0.7845, 'eval_map_jumpsuit': 0.0, 'eval_mar_100_jumpsuit': 0.0, 'eval_map_cape': 0.0, 'eval_mar_100_cape': 0.0, 'eval_map_glasses': 0.0338, 'eval_mar_100_glasses': 0.3209, 'eval_map_hat': 0.133, 'eval_mar_100_hat': 0.4281, 'eval_map_headband, head covering, hair accessory': 0.0005, 'eval_mar_100_headband, head covering, hair accessory': 0.0484, 'eval_map_tie': 0.0, 'eval_mar_100_tie': 0.0, 'eval_map_glove': 0.0, 'eval_mar_100_glove': 0.0, 'eval_map_watch': 0.0012, 'eval_mar_100_watch': 0.0143, 'eval_map_belt': 0.0104, 'eval_mar_100_belt': 0.1479, 'eval_map_leg warmer': 0.0, 'eval_mar_100_leg warmer': 0.0, 'eval_map_tights, stockings': 0.0103, 'eval_mar_100_tights, stockings': 0.1779, 'eval_map_sock': 0.0, 'eval_mar_100_sock': 0.0, 'eval_map_shoe': 0.3485, 'eval_mar_100_shoe': 0.4983, 'eval_map_bag, wallet': 0.0102, 'eval_mar_100_bag, wallet': 0.1192, 'eval_map_scarf': 0.0, 'eval_mar_100_scarf': 0.0, 'eval_map_umbrella': 0.0, 'eval_mar_100_umbrella': 0.0, 'eval_map_hood': 0.0, 'eval_mar_100_hood': 0.0, 'eval_map_collar': 0.0263, 'eval_mar_100_collar': 0.1557, 'eval_map_lapel': 0.0272, 'eval_mar_100_lapel': 0.1957, 'eval_map_epaulette': 0.0, 'eval_mar_100_epaulette': 0.0, 'eval_map_sleeve': 0.236, 'eval_mar_100_sleeve': 0.4591, 'eval_map_pocket': 0.0004, 'eval_mar_100_pocket': 0.0801, 'eval_map_neckline': 0.0733, 'eval_mar_100_neckline': 0.2499, 'eval_map_buckle': 0.0, 'eval_mar_100_buckle': 0.0, 'eval_map_zipper': 0.0001, 'eval_mar_100_zipper': 0.0046, 'eval_map_applique': 0.0, 'eval_mar_100_applique': 0.0, 'eval_map_bead': 0.0, 'eval_mar_100_bead': 0.0, 'eval_map_bow': 0.0, 'eval_mar_100_bow': 0.0, 'eval_map_flower': 0.0, 'eval_mar_100_flower': 0.0, 'eval_map_fringe': 0.0, 'eval_mar_100_fringe': 0.0, 'eval_map_ribbon': 0.0, 'eval_mar_100_ribbon': 0.0, 'eval_map_rivet': 0.0, 'eval_mar_100_rivet': 0.0, 'eval_map_ruffle': 0.0, 'eval_mar_100_ruffle': 0.0, 'eval_map_sequin': 0.0, 'eval_mar_100_sequin': 0.0, 'eval_map_tassel': 0.0, 'eval_mar_100_tassel': 0.0, 'eval_runtime': 46.9685, 'eval_samples_per_second': 21.291, 'eval_steps_per_second': 5.323}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(test_dataset_transformed)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cacdb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5eac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "+--------------------+--------+\n",
      "|       Metric       | Value  |\n",
      "+--------------------+--------+\n",
      "|  Precision (mAP)   | 0.0436 |\n",
      "|  Recall (mAR_100)  | 0.1315 |\n",
      "| Inference Time (s) | 0.047  |\n",
      "+--------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "# metrics = trainer.evaluate(test_dataset_transformed)\n",
    "batch_size = trainer.args.per_device_eval_batch_size\n",
    "print(batch_size)\n",
    "# Extract required values\n",
    "selected_metrics = {\n",
    "    \"Precision (mAP)\": metrics.get(\"eval_map\", 0),\n",
    "    \"Recall (mAR_100)\": metrics.get(\"eval_mar_100\", 0),\n",
    "    \"Inference Time (s)\": 1 / (metrics.get(\"eval_steps_per_second\", 1) * batch_size)\n",
    "}\n",
    "\n",
    "# Create a PrettyTable for display\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Metric\", \"Value\"]\n",
    "\n",
    "# Add metric results to the table\n",
    "for key, value in selected_metrics.items():\n",
    "    table.add_row([key, round(value, 4)])\n",
    "\n",
    "# Print the formatted table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4558d21",
   "metadata": {},
   "source": [
    "# Testing on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dda9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "idx=50 # you can change these image ids to test on different images from test dataset [0, 1000]\n",
    "sample = test_dataset[idx]\n",
    "image = sample[\"image\"]\n",
    "image_bytes = image[\"bytes\"]  # Extract bytes from dictionary\n",
    "image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "#loading fine-tuned model for testing\n",
    "obj_detector = pipeline(\n",
    "    \"object-detection\", model=\"/home/utn/firi22ka/Desktop/jenga/mlp/g10/test/detr-resnet-50-fashionpedia-full-finetuned_5epochs/checkpoint-16300\"  # Change with your model name\n",
    ")\n",
    "\n",
    "\n",
    "results = obj_detector(image)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(image, results, threshold=0.6):\n",
    "    image = Image.fromarray(np.uint8(image))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    width, height = image.size\n",
    "\n",
    "    for result in results:\n",
    "        score = result[\"score\"]\n",
    "        label = result[\"label\"]\n",
    "        box = list(result[\"box\"].values())\n",
    "\n",
    "        if score > threshold:\n",
    "            x1, y1, x2, y2 = tuple(box)\n",
    "            draw.rectangle((x1, y1, x2, y2), outline=\"red\", width=3)\n",
    "            draw.text((x1 + 5, y1 - 10), label, fill=\"white\")\n",
    "            draw.text((x1 + 5, y1 + 10), f\"{score:.2f}\", fill=\"green\" if score > 0.7 else \"red\")\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a559c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(image, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4426aa",
   "metadata": {},
   "source": [
    "# To visualize and save results on mutilple images a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eaf3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def visualy_compare_object_detection(dataset, output_dir= 'result/detr', num_samples=4):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    indices = random.sample(range(len(test_dataset)), num_samples)\n",
    "    print(f'visualizing performance of fine-tuned models on {dataset} images indexed at {indices}')\n",
    "    # Load the pretrained model\n",
    "    obj_detector_pretrained = pipeline(\n",
    "        \"object-detection\", model=\"facebook/detr-resnet-50\"  \n",
    "    )\n",
    "\n",
    "    # Load the fine-tuned model\n",
    "    obj_detector_finetuned = pipeline(\n",
    "        \"object-detection\", model=\"/home/utn/firi22ka/Desktop/jenga/mlp/g10/test/detr-resnet-50-fashionpedia-full-finetuned_5epochs/checkpoint-25000\" \n",
    "    )\n",
    "    for idx in indices:\n",
    "        sample = dataset[idx]\n",
    "        image = sample[\"image\"]\n",
    "        image_bytes = image[\"bytes\"]  # Extract bytes from dictionary\n",
    "        image = Image.open(io.BytesIO(image_bytes))\n",
    "        image_np = np.array(image)\n",
    "\n",
    "        # Object detection using pretrained model\n",
    "        results_pretrained = obj_detector_pretrained(image)\n",
    "        \n",
    "        # # Object detection  using fine-tuned model\n",
    "        results_finetuned = obj_detector_finetuned(image)\n",
    "\n",
    "        # Plotting & Saving ground truth images\n",
    "        ground_truth_with_boxes = draw_image_from_idx(dataset, idx)\n",
    "        ground_truth_with_boxes.save(f\"{output_dir}/{idx}_gt.jpg\")\n",
    "\n",
    "        # Plotting & Saving results for the pretrained model\n",
    "        image_with_pretrained_detections = plot_results(image_np, results_pretrained)\n",
    "        image_with_pretrained_detections.save(f\"{output_dir}/{idx}_pretrained.jpg\")\n",
    "\n",
    "        # # Plotting & Saving results for the fine-tuned model\n",
    "        image_with_finetuned_detections = plot_results(image_np, results_finetuned)\n",
    "        image_with_finetuned_detections.save(f\"{output_dir}/{idx}_finetuned.jpg\")\n",
    "\n",
    "        print(f\"Results for Image {idx} are saved at {output_dir}\")\n",
    "        \n",
    "\n",
    "\n",
    "visualy_compare_object_detection(dataset=test_dataset, output_dir= \"resutl/detr\", num_samples= 4) # change number of sample to check result on tham many images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
